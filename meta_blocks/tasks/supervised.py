"""Unsupervised tasks for meta-learning."""

import logging
from typing import List, Tuple

import numpy as np
import tensorflow.compat.v1 as tf

from meta_blocks import datasets
from meta_blocks.tasks import base

logger = logging.getLogger(__name__)

# Transition to V2 will happen in stages.
tf.disable_v2_behavior()
tf.enable_resource_variables()

__all__ = ["SupervisedTask", "SupervisedTaskDistribution"]


class SupervisedTask(base.Task):
    """
    A classical task for supervised meta-learning.

    The Task is layer above a Dataset generated by a MetaDataset. Essentially,
    Datasets provide access to an infinite flow of data from their underlying
    Categories. Tasks manage that flow and define the following components:
        1) Query set: a balanced sample (1 per class) of labeled examples.
               Technically, this set defines the classes.
        2) Support set: a (large) set of unlabeled examples.
                Basically, the rest of data that are not in the query set.

    Note that even though the task is supervised, the support is unlabeled.
    To get labeled points from the support set, it needs a Sampler (must be
    provided at construction time).
    """

    def __init__(
        self,
        dataset: datasets.Dataset,
        num_query_shots: int = 1,
        name: str = "SupervisedTask",
    ):
        """Instantiates a SupervisedTask.

        Args:
            dataset: Dataset
            num_query_shots: int (default: 1)
            name: str (default: "SupervisedTask")
        """
        super(SupervisedTask, self).__init__(
            dataset=dataset, num_query_shots=num_query_shots, name=name
        )

        # Internals.
        self._preprocessor = None
        self._support_labeled_ids = None
        self._support_inputs_raw = None
        self._support_labels_raw = None
        self._support_inputs = None
        self._support_labels = None
        self._support_tensors = None
        self._query_tensors = None

    @property
    def support_size(self) -> tf.Tensor:
        return tf.size(self._support_labeled_ids)

    @property
    def unlabeled_support_size(self) -> tf.Tensor:
        return tf.shape(self.unlabeled_support_inputs_raw)[0]

    @property
    def support_tensors(self):
        return self._support_tensors

    @property
    def query_tensors(self):
        return self._query_tensors

    @property
    def unlabeled_support_inputs(self):
        """Preprocessed unlabeled support inputs."""
        return self._support_inputs

    @property
    def unlabeled_support_inputs_raw(self):
        """Unpreprocessed unlabeled support inputs."""
        return self._support_inputs_raw

    def _get_inputs_and_labels(self, start_index, end_index):
        """Slices provides tensors and returns inputs, labels, and kwargs."""
        inputs = [x[start_index:end_index] for x in self.dataset.data_tensors]
        labels = [tf.fill(tf.shape(x)[:1], k) for k, x in enumerate(inputs)]
        return tf.concat(inputs, axis=0), tf.concat(labels, axis=0)

    def _preprocess(self, inputs, labels, back_prop=False, parallel_iterations=16):
        if self._preprocessor is not None:
            inputs = tf.map_fn(
                dtype=tf.float32,
                fn=self._preprocessor,
                elems=(inputs, labels),
                back_prop=back_prop,
                parallel_iterations=parallel_iterations,
            )
        return inputs, labels

    def _build(self):
        """Builds the task internals in the correct name scope."""
        # Input preprocessor.
        self._preprocessor = self.dataset.get_preprocessor()
        # Build placeholder for labeled support ids.
        self._support_labeled_ids = tf.placeholder(
            tf.int32, shape=[None], name="support_labeled_ids"
        )
        # Build query tensors.
        start_index = 0
        end_index = start_index + self.num_query_shots
        inputs, labels = self._get_inputs_and_labels(start_index, end_index)
        self._query_tensors = self._preprocess(inputs, labels)
        # Build support tensors.
        start_index = end_index
        (
            self._support_inputs_raw,
            self._support_labels_raw,
        ) = self._get_inputs_and_labels(start_index, None)
        self._support_inputs, self._support_labels = self._preprocess(
            self._support_inputs_raw, self._support_labels_raw
        )
        self._support_tensors_raw = tuple(
            map(
                lambda x: tf.gather(x, self._support_labeled_ids, axis=0),
                (self._support_inputs_raw, self._support_labels_raw),
            )
        )
        self._support_tensors = self._preprocess(*self._support_tensors_raw)

    def get_feed_list(self, support_labeled_ids):
        """Creates a feed list needed for the task."""
        feed_list = [(self._support_labeled_ids, support_labeled_ids)]
        return feed_list


class SupervisedTaskDistribution(base.TaskDistribution):
    """A distribution that provides access to supervised tasks."""

    def __init__(
        self,
        meta_dataset: datasets.MetaDataset,
        num_query_shots: int = 1,
        num_support_shots: int = 1,
        max_labeled_points: int = None,
        init_labeled_points: int = None,
        name: str = "SupervisedTaskDistribution",
        seed: int = 42,
    ):
        super(SupervisedTaskDistribution, self).__init__(
            meta_dataset=meta_dataset, num_query_shots=num_query_shots, name=name
        )
        self.num_support_shots = num_support_shots
        self.max_labeled_points = max_labeled_points
        self.init_labeled_points = init_labeled_points

        # Setup random number generator.
        self._rng = np.random.RandomState(seed=seed)

        # Internals.
        self._requests = None
        self._requested_ids = None
        self._requested_kwargs = None
        self._requested_labels = None
        self._sampler = None

    @property
    def sampler(self):
        return self._sampler

    @property
    def query_labels_per_task(self):
        return self.num_classes * self.num_query_shots

    @property
    def support_labels_per_task(self):
        return self.num_classes * self.num_support_shots

    @property
    def requested_labels(self):
        return self._requested_labels

    def _build(self):
        # Build a batch of tasks.
        self._task_batch = tuple(
            SupervisedTask(
                dataset=dataset,
                num_query_shots=self.num_query_shots,
                name=f"SupervisedTask_{i}",
            ).build()
            for i, dataset in enumerate(self.meta_dataset.dataset_batch)
        )

    def initialize(self, sess: tf.Session, sampler=None, **kwargs):
        """Initializes by pre-sampling supervised tasks."""
        if sampler is not None:
            self._sampler = sampler

        # Determine the initial labeling budget.
        if self.init_labeled_points is None:
            self.init_labeled_points = self.max_labeled_points

        # Reset.
        self._requests = []
        self._requested_ids = []
        self._requested_kwargs = []
        self._requested_labels = 0

        # Sample supervised tasks.
        logger.info(f"Initializing {self.name}...")
        self.expand(self.init_labeled_points, sess=sess, **kwargs)

    def expand(self, num_labeled_points: int, sess: tf.Session, **kwargs):
        """Expands the number of labeled points by sampling more tasks."""
        # Never exceed the hard budget.
        num_labeled_points = min(num_labeled_points, self.max_labeled_points)

        # Expand the number of requested labels.
        requested_labels_so_far = self._requested_labels
        while requested_labels_so_far < num_labeled_points:
            logger.debug(
                f"...requesting more labels: "
                f"{requested_labels_so_far}/{num_labeled_points}"
            )
            # Construct a batch of requests.
            requests_batch = tuple(
                tuple(
                    self._rng.choice(
                        self.meta_dataset.num_categories,
                        size=self.num_classes,
                        replace=False,
                    )
                )
                for _ in range(self.meta_batch_size)
            )
            feed_list_batch = self.meta_dataset.get_feed_list_batch(requests_batch)
            # Get request kwargs.
            request_kwargs_batch = [
                [v for _, v in feed_list[self.num_classes :]]
                for feed_list in feed_list_batch
            ]
            # Sample support labeled ids for the requested tasks.
            support_labeled_ids_batch = self._sampler.select_labeled(
                size=self.support_labels_per_task,
                labels_per_step=self.num_classes,
                sess=sess,
                feed_dict=dict(sum(feed_list_batch, [])),
                **kwargs,
            )
            # Save sampled information.
            for i, ids in enumerate(support_labeled_ids_batch):
                requested_labels_so_far += self.query_labels_per_task + len(ids)
                if requested_labels_so_far > num_labeled_points:
                    break
                self._requested_ids.append(ids)
                self._requests.append(requests_batch[i])
                self._requested_kwargs.append(request_kwargs_batch[i])
                self._requested_labels = requested_labels_so_far

    def sample_task_feed(self, replace: bool = True) -> List[Tuple[tf.Tensor, str]]:
        """Samples a meta-batch of tasks and returns a feed list."""
        # Sample a meta-batch of tasks.
        indices_batch = self._rng.choice(
            len(self._requests), size=self.meta_batch_size, replace=replace
        )
        # Build feed list for the meta-batch of tasks.
        requests_batch = [self._requests[i] for i in indices_batch]
        ids_batch = [self._requested_ids[i] for i in indices_batch]
        kwargs_batch = [self._requested_kwargs[i] for i in indices_batch]
        feed_list_batch = self.meta_dataset.get_feed_list_batch(requests_batch)
        task_feed = []
        for i, feed_list in enumerate(feed_list_batch):
            # Truncate feed list.
            task_feed += feed_list[: self.num_classes]
            # Add kwargs feed.
            kwarg_keys = [k for k, _ in feed_list[self.num_classes :]]
            task_feed += [(k, v) for k, v in zip(kwarg_keys, kwargs_batch[i])]
            # Add task-specific feed.
            task_feed += self._task_batch[i].get_feed_list(ids_batch[i])
        return task_feed
